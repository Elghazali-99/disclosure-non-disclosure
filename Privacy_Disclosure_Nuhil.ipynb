{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Sure the Current Python Environment is Right One :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/nuhil/miniconda3/envs/nlp/bin/python'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Data Crawler from http://www.medhelp.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from os import listdir\n",
    "# import sys\n",
    "\n",
    "# def trade_spider(url):\n",
    "#     source_code = requests.get(url)\n",
    "#     plain_text = source_code.text\n",
    "#     soup = BeautifulSoup(plain_text, \"html.parser\")\n",
    "\n",
    "#     content = soup.find('div', {'class': 'post_message'})\n",
    "#     content = content.text if content else \"\"\n",
    "#     return content\n",
    "\n",
    "# # load doc into memory\n",
    "# def load_doc(filename):\n",
    "#     file_to_work = open(filename, \"r\")\n",
    "\n",
    "#     urls = []\n",
    "#     for my_line in file_to_work:\n",
    "#         urls.append(my_line)\n",
    "\n",
    "#     file_to_work.close()\n",
    "#     return urls    \n",
    "\n",
    "# urls = load_doc(\"Data_Sources/Medical_Post_URLs/Divorce--Breakups.txt\")\n",
    "\n",
    "# i = 1180\n",
    "# for url in urls:\n",
    "#     if i == 2181:\n",
    "#         break\n",
    "        \n",
    "#     post = trade_spider(url)\n",
    "#     if len(post) < 1000:\n",
    "#         continue\n",
    "        \n",
    "#     with open(\"Disclose_Nondisclose/private/\"+str(i).zfill(5)+\"_divorce-breakups.txt\", \"w\") as f:\n",
    "#         f.write(post.strip())\n",
    "        \n",
    "#     i += 1 \n",
    "#     print(i)\n",
    "    \n",
    "# print(\"Done!!!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wiki Article Processor Using https://github.com/attardi/wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# file = open(\"AA/wiki_XX\", encoding = \"utf8\") \n",
    "# data = file.readlines()\n",
    "# wiki = pd.DataFrame()\n",
    "\n",
    "# i = 0\n",
    "# for js in data:\n",
    "#     wikidata = json.loads(js)\n",
    "    \n",
    "#     with open(str(i).zfill(5)+\"_\"+re.sub(\"\\W+\",\"-\", wikidata['title'].strip().lower())+\".txt\", \"w\") as f:\n",
    "#         f.write(wikidata['text'].strip())\n",
    "\n",
    "#     i += 1     \n",
    "    \n",
    "# print(\"Done!!!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Randomize File Contents in a Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from os import listdir\n",
    "\n",
    "# # load doc into memory\n",
    "# def load_doc(filename):\n",
    "#     # open the file as read only\n",
    "#     file = open(filename, 'r')\n",
    "#     # read all text\n",
    "#     text = file.read()\n",
    "#     # close the file\n",
    "#     file.close()\n",
    "#     return text\n",
    "\n",
    "# # specify directory to load\n",
    "# directory = 'private'\n",
    "# # walk through all files in the folder\n",
    "# i = 0\n",
    "# for filename in listdir(directory):\n",
    "#     # skip files that do not have the right extension\n",
    "#     if not filename.endswith(\".txt\"):\n",
    "#         continue\n",
    "        \n",
    "#     # create the full path of the file to open\n",
    "#     path = directory + '/' + filename\n",
    "#     # load document\n",
    "#     doc = load_doc(path)\n",
    "#     with open(\"private_r/\"+str(i).zfill(5)+\"_\"+filename.split(\"_\")[1], \"w\") as f:\n",
    "#         f.write(doc.strip())\n",
    "    \n",
    "#     i += 1\n",
    "        \n",
    "# print('Done!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load doc's content into memory by it's name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \"\"\"\n",
    "    Load a doc into memory by its name\n",
    "    \"\"\"\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turn a given doc into clean tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Turn a doc into clean tokens\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process each document inside a directory and add to `vocab` counter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        # skip any posts in the test set\n",
    "        # test set starts from 04000_CATE-GORY.txt\n",
    "        if is_trian and filename.startswith('04'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('04'):\n",
    "            continue\n",
    "            \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process all documents to add everything into `vocab` Counter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102608\n",
      "[('The', 27440), ('like', 11208), ('people', 10646), ('also', 10114), ('He', 9445), ('It', 9288), ('In', 9269), ('one', 8641), ('time', 8423), ('would', 7522), ('get', 7253), ('called', 7188), ('Im', 6845), ('years', 6685), ('dont', 6607), ('This', 6435), ('many', 6353), ('know', 6244), ('feel', 6189), ('first', 5622), ('used', 5418), ('made', 5340), ('They', 5288), ('things', 4775), ('two', 4708), ('said', 4679), ('even', 4662), ('want', 4629), ('go', 4515), ('make', 4391), ('back', 4388), ('could', 4129), ('much', 4111), ('year', 4099), ('life', 4065), ('really', 4042), ('help', 4009), ('think', 3979), ('day', 3832), ('never', 3820), ('My', 3793), ('She', 3778), ('started', 3681), ('different', 3649), ('still', 3604), ('way', 3551), ('There', 3471), ('going', 3428), ('work', 3366), ('around', 3241)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('Data/Disclose_Nondisclose/public', vocab, True)\n",
    "process_docs('Data/Disclose_Nondisclose/private', vocab, True)\n",
    "\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove tokens that appeared less than twice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50478\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save final vocab/tokens to a file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'Data/Parsed_Vocab/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with Trained Word Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similar to the above `clean_doc` function but it filters out tokens that are not in `vocab`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similar to the above `process_docs` function but it does not save tokens to a file, rather keeps in memory named `documents`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue        \n",
    "            \n",
    "        # skip any posts in the test set\n",
    "        # test set starts from 04000_CATE-GORY.txt\n",
    "        if is_trian and filename.startswith('04'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('04'):\n",
    "            continue\n",
    "            \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        \n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50478\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load all training posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_docs = process_docs('Data/Disclose_Nondisclose/private', vocab, True)\n",
    "negative_docs = process_docs('Data/Disclose_Nondisclose/public', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    "\n",
    "# print(type(train_docs))\n",
    "# print(len(train_docs))\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding to keep all docs in a certain size. Also prepare training data set and associated labels as Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad sequences\n",
    "# get the biggest post as per its contents\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "\n",
    "# define training data\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "# define training labels\n",
    "# put 0s for the first 4000 entries and 1s for last 4000 entries.\n",
    "# Because, in the <train_docs> list we have the public docs first\n",
    "# and private docs later.\n",
    "# From now on we are assuming 0 for public data, 1 for private data\n",
    "ytrain = array([0 for _ in range(4000)] + [1 for _ in range(4000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding to keep all docs in a certain size. Also prepare test data set and associated labels as Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all test posts\n",
    "positive_docs = process_docs('Data/Disclose_Nondisclose/private', vocab, False)\n",
    "negative_docs = process_docs('Data/Disclose_Nondisclose/public', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(1000)] + [1 for _ in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41952\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the final ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 3586, 100)         4195200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 3579, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 1789, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 57248)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                572490    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,793,333\n",
      "Trainable params: 4,793,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the ANN and Evaluate on top of test data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "295s - loss: 0.1223 - acc: 0.9245\n",
      "Epoch 2/10\n",
      "348s - loss: 0.0032 - acc: 0.9991\n",
      "Epoch 3/10\n",
      "390s - loss: 7.7224e-04 - acc: 0.9998\n",
      "Epoch 4/10\n",
      "358s - loss: 4.3293e-05 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "334s - loss: 1.1118e-05 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "367s - loss: 6.1526e-06 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "336s - loss: 4.1034e-06 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "313s - loss: 2.9994e-06 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "319s - loss: 2.3187e-06 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "316s - loss: 1.8539e-06 - acc: 1.0000\n",
      "Test Accuracy: 98.700000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    \"\"\"\n",
    "    Turn a doc into clean tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_new_doc(path):\n",
    "    \"\"\"\n",
    "    Process a single doc with post\n",
    "    \"\"\"\n",
    "    # define a new list\n",
    "    documents = list()\n",
    "    \n",
    "    # load doc\n",
    "    doc = load_doc(path)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # add to list\n",
    "    documents.append(tokens)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_privacy(path, max_length, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Give a new unseen doc to predict it's privacy\n",
    "    \"\"\"\n",
    "    new_doc = process_new_doc(path)\n",
    "    # sequence encode\n",
    "    # To Do:\n",
    "    # 1. Will check the current <tokentizer> in memory\n",
    "    encoded_doc = tokenizer.texts_to_sequences(new_doc)\n",
    "    \n",
    "    # pad sequences\n",
    "    max_length = max_length\n",
    "    Xpredict = pad_sequences(encoded_doc, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # prediction\n",
    "    ypredict = model.predict(Xpredict, verbose=1)\n",
    "    print(\"Privacy Score: {0} \\nRounded To: {1}\".format(ypredict, round(ypredict[0,0])))\n",
    "    \n",
    "    return round(ypredict[0,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give any custom text file to predict the privacy of its content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "Privacy Score: [[ 0.99999666]] \n",
      "Rounded To: 1.0\n",
      "\n",
      "Content is Private\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nContent is Public\" if predict_privacy('Data/Disclose_Nondisclose/predict/abuse-support.txt', max_length, tokenizer, model) == 0.0 else \"\\nContent is Private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
